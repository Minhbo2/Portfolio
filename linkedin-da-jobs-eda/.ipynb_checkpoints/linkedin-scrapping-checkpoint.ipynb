{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f4fc812-4908-42a8-9095-7126dcc2e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21270a07-bc16-41f4-a04f-3c1a3b52cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5499d9-c3f5-415e-b549-cc04266f65c4",
   "metadata": {},
   "source": [
    "## Prepping before the scrape\n",
    "- load the correct site with query search pre-filled\n",
    "- pre-load number of job posting(starting at 1000 posts)\n",
    "- define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0848f4-ce6b-4f1b-b569-842f36248117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to download the correct chrome driver, update path to its directory\n",
    "wd = webdriver.Chrome(executable_path='./chromedriver.exe')\n",
    "url = 'https://www.linkedin.com/jobs/search?keywords=Data%20Analyst&location=San%20Francisco%20Bay%20Area&locationId=&geoId=90000084&sortBy=R&f_TPR=&f_E=2%2C3&position=1&pageNum=0'\n",
    "wd.get(url)\n",
    "wd.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "950f64df-b695-46c7-b32e-8a76f7e2d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert str result of job counts into int\n",
    "no_of_jobs = wd.find_element_by_css_selector('h1>span').get_attribute('innerText')\n",
    "no_of_jobs_str = no_of_jobs\n",
    "no_of_jobs_str = no_of_jobs_str[:-1]\n",
    "no_of_jobs_int = int(no_of_jobs_str.replace(',',''))\n",
    "no_of_jobs_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c373200a-69c9-4984-a277-e979387e5b2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expanding the listing to get a total of no_of_jobs_in posts or 1000 posts\n",
    "i = 2\n",
    "\n",
    "# while i <= int(no_of_jobs_int/25) + 1:\n",
    "while i <= int(1000/25) + 1: # just want about 1000 postings\n",
    "    wd.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "    i = i+1\n",
    "    try:\n",
    "        wd.find_element_by_class_name('infinite-scroller__show-more-button.infinite-scroller__show-more-button').click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        pass\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc6315d-a601-40eb-91b6-5abb6ed92791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out unwanted job posts\n",
    "def jobIsNotWanted(title):\n",
    "    if re.search('analyst', title.lower()):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc1edc7f-714b-448b-821b-af7315141862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed1502c6-6cd9-4a60-933c-00ba5776a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the show more button to expand the job post description html panel \n",
    "def clickShowMore(section):\n",
    "    buttons = section.find_elements_by_tag_name('button')\n",
    "    \n",
    "    if not len(buttons) < 1:\n",
    "        for button in buttons:\n",
    "            try:\n",
    "                if button.text == 'Show more':\n",
    "                    button.click()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853e525-8ec0-4fda-961f-58f627b7cb28",
   "metadata": {},
   "source": [
    "## Scraping process:\n",
    "- defined desire fields(id, title, company, location, description).\n",
    "- grab all job posting and iterate over each post\n",
    "- grab and fill the defined fields\n",
    "- skip over any posting without a populated description panel or missing show more button since they are incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9f78ff8-f82c-40f7-aec9-27b99cf2e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fields\n",
    "job_id = []\n",
    "job_title = []\n",
    "company_name = []\n",
    "location = []\n",
    "description = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d7e27261-a1ba-4a30-ba3f-d57c31818e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### use this block if list expanded to total posting before hand\n",
    "job_listing = wd.find_element_by_class_name('jobs-search__results-list')\n",
    "jobs = job_listing.find_elements_by_tag_name('li')\n",
    "\n",
    "for job in jobs:\n",
    "    # click on each li ele to populate the job card \n",
    "    job.click()\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    section = wd.find_element_by_xpath('/html/body/div[1]/div/section')\n",
    "    clickShowMore(section)\n",
    "\n",
    "#     get job id and title, filtering out existing and non-desirable job based on title\n",
    "    url = wd.current_url\n",
    "    url_parts = url.split('JobId=')\n",
    "    title = job.find_element_by_css_selector('div>h3')\n",
    "    # move on to next iteration if already exist and job is not wanted\n",
    "    if((url_parts[1] in job_id or jobIsNotWanted(title.text)) and len(section.text) <= 1): \n",
    "        continue\n",
    "        \n",
    "    job_id.append(url_parts[1])\n",
    "    job_title.append(title.text)\n",
    "\n",
    "    # get the company name\n",
    "    company = job.find_element_by_css_selector('div>h4')\n",
    "    company_name.append(company.text)\n",
    "\n",
    "    # job location\n",
    "    job_location = job.find_element_by_class_name('job-search-card__location')\n",
    "    location.append(job_location.text)\n",
    "    \n",
    "    # job description\n",
    "    desc = section.text\n",
    "    description[url_parts[1]] = desc\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c3ade58-c51a-47e0-95b4-f83cc711d2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615\n",
      "615\n"
     ]
    }
   ],
   "source": [
    "print(len(job_id))\n",
    "print(len(description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea412e-dbcb-4c09-90e7-43d969f2596d",
   "metadata": {},
   "source": [
    "## Converting data to DF and read/write to .csv files\n",
    "- save to file so they can be work on at a later time.\n",
    "- preventing repeating the process if unable to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fec6c9d2-6d7c-46d5-b2f9-69d635c278a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = pd.DataFrame()\n",
    "\n",
    "job_dict = {\n",
    "    'id': job_id,\n",
    "    'title': job_title,\n",
    "    'company_name': company_name,\n",
    "    'location': location\n",
    "}\n",
    "\n",
    "job_df = pd.DataFrame(job_dict)\n",
    "job_df.to_csv('jobs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5a9283d5-1e7f-4354-a85d-956730d45a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc_df = pd.DataFrame.from_dict({'job_id': description.keys(), 'description':description.values()})\n",
    "job_desc_df.to_csv('jobs_desc.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165d8a0-a9e5-412c-a4e0-a409572b844c",
   "metadata": {},
   "source": [
    "#### Load files upon restarting job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f76927-7fc6-4ee5-a476-c11e94085c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files when restarting job\n",
    "jobs_df = pd.read_csv('jobs.csv')\n",
    "jobs_desc_df = pd.read_csv('jobs_desc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c912118-38e0-4321-bd4c-17cc81f6018f",
   "metadata": {},
   "source": [
    "# Scrubbing and handling null values\n",
    "- find city zipcode from job csv\n",
    "- remove duplicates, if any, and remove all null for both job/job_desc csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ec2b0bb-0f53-4e3f-ad4f-ae64c772dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# add zipcode to job csv \n",
    "# remove rows from job_desc and job csv files if description field is null\n",
    "# sql, python, R, Tableau, power BI, excel, looker, powerpoint, SAS, ETL, Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9cb07c5-d4f9-48f4-b91a-6478ffb8d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n"
     ]
    }
   ],
   "source": [
    "# drop nulls from desc csv\n",
    "non_null_jobs_desc = pd.DataFrame(jobs_desc_df.dropna(how = 'any'))\n",
    "desc_id = non_null_jobs_desc.index\n",
    "non_null_jobs_desc['desc_id'] = desc_id\n",
    "non_null_jobs_desc.to_csv('new_jobs_desc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a2c438b-540b-4686-8c10-e1153b2f2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows from job_id csv if they have been drop in desc csv\n",
    "temp_jobs_df = pd.DataFrame(jobs_df)\n",
    "temp_job_id = jobs_df['id'].values\n",
    "job_id_from_desc = non_null_jobs_desc['job_id'].values\n",
    "\n",
    "for i in temp_job_id:\n",
    "    if not i in job_id_from_desc:\n",
    "        row = (temp_jobs_df.loc[temp_jobs_df['id'] == i]).index\n",
    "        temp_jobs_df.drop(index = row, inplace = True)\n",
    "\n",
    "temp_jobs_df.to_csv('new_jobs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80b231ee-636a-4fa8-a95a-aee84598f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = pd.read_csv('new_jobs.csv')\n",
    "jobs_desc_df = pd.read_csv('new_jobs_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac3a267-e8fa-4fe9-b569-439bc15ea9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uszipcode import SearchEngine, SimpleZipcode, Zipcode\n",
    "\n",
    "# zipcode finder helper function\n",
    "def getZipcode(city_name):\n",
    "    list_zipcode = []\n",
    "    search = SearchEngine()\n",
    "    zipcode = search.by_city_and_state(city_name, 'CA')\n",
    "    if len(zipcode) < 1:\n",
    "        return None\n",
    "    return zipcode[0].zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6682d799-4d73-4ae4-b7a7-58c086d1f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_locations = job_df['location'].values\n",
    "location_zipcode = []\n",
    "for location in job_locations:\n",
    "    zipc = getZipcode(location)\n",
    "    location_zipcode.append(zipc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db720289-b347-4254-903c-01c0894b368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df['zipcode'] = location_zipcode\n",
    "job_df.to_csv('new_jobs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ab4c0-b427-436f-8356-f1a5069a7891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e58617-4b66-4f9a-947d-00bc13436b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrubbing null values for job level\n",
    "# missing_seniority_level = seniority_level\n",
    "# missing_seniority_level = [i if len(i) > 0 else 'Not Applicable' for i in seniority_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf91ce1-0f9d-4b7c-9d53-fcb1e6e40e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate city and state\n",
    "# state = []\n",
    "# city = []\n",
    "\n",
    "# for i in range(len(location)):\n",
    "#     state.append(location[i][-2:])\n",
    "#     city.append(location[i][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2b37da-29da-49ec-802f-e57fc1959fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing , in company name, just personal ref\n",
    "# cleaned_company_name = [name.replace(',','-') for name in company_name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
