{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping before the scrape\n",
    "- load the correct site with query search pre-filled\n",
    "- pre-load number of job posting(starting at 1000 posts)\n",
    "- define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to download the correct chrome driver, update path to its directory\n",
    "wd = webdriver.Chrome(executable_path='./chromedriver.exe')\n",
    "url = 'https://www.linkedin.com/jobs/search?keywords=Data%20Analyst&location=San%20Francisco%20Bay%20Area&locationId=&geoId=90000084&sortBy=R&f_TPR=&f_E=2%2C3&position=1&pageNum=0'\n",
    "wd.get(url)\n",
    "wd.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert str result of job counts into int\n",
    "no_of_jobs = wd.find_element_by_css_selector('h1>span').get_attribute('innerText')\n",
    "no_of_jobs_str = no_of_jobs\n",
    "no_of_jobs_str = no_of_jobs_str[:-1]\n",
    "no_of_jobs_int = int(no_of_jobs_str.replace(',',''))\n",
    "no_of_jobs_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expanding the listing to get a total of no_of_jobs_in posts or 1000 posts\n",
    "i = 2\n",
    "\n",
    "# while i <= int(no_of_jobs_int/25) + 1:\n",
    "while i <= int(1000/25) + 1: # just want about 1000 postings\n",
    "    wd.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "    i = i+1\n",
    "    try:\n",
    "        wd.find_element_by_class_name('infinite-scroller__show-more-button.infinite-scroller__show-more-button').click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        pass\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out unwanted job posts\n",
    "def jobIsNotWanted(title):\n",
    "    if re.search('analyst', title.lower()):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the show more button to expand the job post description html panel \n",
    "def clickShowMore(section):\n",
    "    buttons = section.find_elements_by_tag_name('button')\n",
    "    \n",
    "    if not len(buttons) < 1:\n",
    "        for button in buttons:\n",
    "            try:\n",
    "                if button.text == 'Show more':\n",
    "                    button.click()\n",
    "                    break\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping process:\n",
    "- defined desire fields(id, title, company, location, description).\n",
    "- grab all job posting and iterate over each post\n",
    "- grab and fill the defined fields\n",
    "- skip over any posting without a populated description panel or missing show more button since they are incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fields\n",
    "job_id = []\n",
    "job_title = []\n",
    "company_name = []\n",
    "location = []\n",
    "description = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### use this block if list expanded to total posting before hand\n",
    "job_listing = wd.find_element_by_class_name('jobs-search__results-list')\n",
    "jobs = job_listing.find_elements_by_tag_name('li')\n",
    "\n",
    "for job in jobs:\n",
    "    # click on each li ele to populate the job card \n",
    "    job.click()\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    section = wd.find_element_by_xpath('/html/body/div[1]/div/section')\n",
    "    clickShowMore(section)\n",
    "\n",
    "#     get job id and title, filtering out existing and non-desirable job based on title\n",
    "    url = wd.current_url\n",
    "    url_parts = url.split('JobId=')\n",
    "    title = job.find_element_by_css_selector('div>h3')\n",
    "    # move on to next iteration if already exist and job is not wanted\n",
    "    if((url_parts[1] in job_id or jobIsNotWanted(title.text)) and len(section.text) <= 1): \n",
    "        continue\n",
    "        \n",
    "    job_id.append(url_parts[1])\n",
    "    job_title.append(title.text)\n",
    "\n",
    "    # get the company name\n",
    "    company = job.find_element_by_css_selector('div>h4')\n",
    "    company_name.append(company.text)\n",
    "\n",
    "    # job location\n",
    "    job_location = job.find_element_by_class_name('job-search-card__location')\n",
    "    location.append(job_location.text)\n",
    "    \n",
    "    # job description\n",
    "    desc = section.text\n",
    "    description[url_parts[1]] = desc\n",
    "    \n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to DF and read/write to .csv files\n",
    "- save to file so they can be work on at a later time.\n",
    "- preventing repeating the process if unable to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = pd.DataFrame()\n",
    "\n",
    "job_dict = {\n",
    "    'id': job_id,\n",
    "    'title': job_title,\n",
    "    'company_name': company_name,\n",
    "    'location': location\n",
    "}\n",
    "\n",
    "job_df = pd.DataFrame(job_dict)\n",
    "job_df.to_csv('jobs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc_df = pd.DataFrame.from_dict({'job_id': description.keys(), 'description':description.values()})\n",
    "job_desc_df.to_csv('jobs_desc.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing and handling null values\n",
    "- find city zipcode from job csv\n",
    "- remove duplicates, if any, and remove all null for both job/job_desc csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486\n"
     ]
    }
   ],
   "source": [
    "# drop nulls from desc csv\n",
    "non_null_jobs_desc = pd.DataFrame(jobs_desc_df.dropna(how = 'any'))\n",
    "desc_id = non_null_jobs_desc.index\n",
    "non_null_jobs_desc['desc_id'] = desc_id\n",
    "non_null_jobs_desc.to_csv('new_jobs_desc.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows from job_id csv if they have been drop in desc csv\n",
    "new_jobs_df = pd.DataFrame(jobs_df)\n",
    "temp_job_id = jobs_df['id'].values\n",
    "job_id_from_desc = non_null_jobs_desc['job_id'].values\n",
    "\n",
    "for i in temp_job_id:\n",
    "    if not i in job_id_from_desc:\n",
    "        row = (new_jobs_df.loc[temp_jobs_df['id'] == i]).index\n",
    "        new_jobs_df.drop(index = row, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uszipcode import SearchEngine, SimpleZipcode, Zipcode\n",
    "\n",
    "# zipcode finder helper function\n",
    "def getZipcode(city_name):\n",
    "    list_zipcode = []\n",
    "    search = SearchEngine()\n",
    "    zipcode = search.by_city_and_state(city_name, 'CA')\n",
    "    if len(zipcode) < 1:\n",
    "        return None\n",
    "    return zipcode[0].zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate city name and state\n",
    "city_state = new_jobs_df['location'].values\n",
    "city = []\n",
    "state = []\n",
    "\n",
    "for city_name in city_state:\n",
    "    if ', CA' in city_name:\n",
    "        location_parts = city_name.split(', ')\n",
    "    else:\n",
    "        location_parts[0] = 'San Francisco'\n",
    "        location_parts[1] = 'CA'\n",
    "    city.append(location_parts[0])\n",
    "    state.append(location_parts[1])\n",
    "\n",
    "new_jobs_df['city'] = city\n",
    "new_jobs_df['state'] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting zipcode\n",
    "job_locations = new_jobs_df['location'].values\n",
    "location_zipcode = []\n",
    "\n",
    "for location in job_locations:\n",
    "    zipc = getZipcode(location)\n",
    "    location_zipcode.append(zipc)\n",
    "\n",
    "new_jobs_df['zipcode'] = location_zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrubbing null values for job level\n",
    "# missing_seniority_level = seniority_level\n",
    "# missing_seniority_level = [i if len(i) > 0 else 'Not Applicable' for i in seniority_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing , in company name, just personal ref\n",
    "# cleaned_company_name = [name.replace(',','-') for name in company_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to new csv\n",
    "new_jobs_df.to_csv('new_jobs.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating skill csv file\n",
    "- check and create csv file including skills list:\n",
    "    - SQL, python, R, Tableau, Power BI, Excel, Looker, Powerpoint, SAS, ETL, Jupyter Notebooks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = pd.read_csv('new_jobs.csv')\n",
    "jobs_desc_df = pd.read_csv('new_jobs_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: checking for R will be a bit tricky, probly will need regex\n",
    "# helper functions to check for skill in list\n",
    "# # if/elif, since python doesnt have switch statement\n",
    "# Resource: https://towardsdatascience.com/how-to-use-nlp-in-python-a-practical-step-by-step-example-bd82ca2d2e1e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
